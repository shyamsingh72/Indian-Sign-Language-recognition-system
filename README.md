ğŸ¤Ÿ Indian Sign Language Recognition System (ISL)

A real-time Indian Sign Language (ISL) recognition system that detects hand gestures using a webcam and converts them into text and voice output. This project aims to reduce the communication gap between hearing-impaired individuals and the general public using Computer Vision and Deep Learning.

ğŸ“Œ Project Overview

Indian Sign Language (ISL) is a primary mode of communication for many hearing-impaired individuals. However, most people are not familiar with ISL, creating communication barriers.
This project uses MediaPipe Hand Landmarks and a Deep Learning model to recognize ISL alphabets (Aâ€“Z) in real time and provide spoken output for better accessibility.


âœ¨ Features

âœ… Real-time ISL alphabet recognition (Aâ€“Z)

ğŸ¤² Supports single-hand and double-hand gestures

ğŸ¥ Live webcam-based detection

ğŸ§  Deep Learningâ€“based classification model

ğŸ”Š Text-to-Speech voice output

ğŸ¨ Color-coded landmarks

ğŸ”´ Left hand

ğŸŸ¢ Right hand

ğŸ’» Low-cost system (only webcam required)



ğŸ› ï¸ Technologies Used

Programming Language: Python

Computer Vision: OpenCV

Hand Tracking: MediaPipe

Deep Learning: TensorFlow / Keras

Text-to-Speech: pyttsx3

Model Format: .h5 / .keras

ğŸ“‚ Project Structure

â”œâ”€â”€ collect_data.py # Collect ISL hand landmark data
â”œâ”€â”€ train_model.py           # Train deep learning model
â”œâ”€â”€ real_time_detection.py   # Real-time ISL recognition + voice
â”œâ”€â”€ X.npy                    # Feature dataset (hand landmarks)
â”œâ”€â”€ y.npy                    # Labels dataset
â”œâ”€â”€ isl_mediapipe_AZ.keras   # Trained model
â”œâ”€â”€ README.md



âš™ï¸ How It Works

Data Collection

Hand landmarks are captured using MediaPipe.

Landmarks are stored in X.npy and labels in y.npy.

Model Training

A neural network is trained on the collected landmark data.

The trained model is saved for real-time prediction.

Real-Time Detection

Webcam input is processed.

Hand landmarks are extracted and passed to the model.

The predicted ISL alphabet is displayed and spoken aloud.



â–¶ï¸ How to Run

1ï¸âƒ£ Install Dependencies
pip install opencv-python mediapipe tensorflow pyttsx3 numpy

2ï¸âƒ£ Collect Data
python collect_data.py

3ï¸âƒ£ Train Model
python train_model.py

4ï¸âƒ£ Run Real-Time Detection
python real_time_detection.py

Press q to exit.



ğŸ“Š Dataset Details

X.npy â†’ Hand landmark coordinates (84 values for two hands)

y.npy â†’ Corresponding alphabet labels

Each sample represents one ISL gesture



ğŸ¯ Applications

Assistive technology for hearing-impaired individuals

Educational tools for learning ISL

Humanâ€“Computer Interaction (HCI)

Accessibility systems

Gesture-based interfaces



ğŸš€ Future Enhancements

Word and sentence-level recognition

Mobile app using TensorFlow Lite

Cloud-based ISL recognition API

Support for dynamic gestures

Integration with smart devices
